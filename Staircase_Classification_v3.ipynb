{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google's Inception-ResNet v2 to Classify Staircases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'logs/Stairs/*': No such file or directory\r\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-889b526aa520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#Load X and Y Lists from Pickle Files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mx_bfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0my_bfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "from keras.utils import plot_model\n",
    "from keras import layers\n",
    "\n",
    "#Static Variables\n",
    "test_data_percentage = 0.1\n",
    "class_names = [\"curved\", \"negative\", \"straight\"]\n",
    "epochs = 30\n",
    "\n",
    "#Tensorboard\n",
    "!rm logs/Stairs/*\n",
    "NAME = \"Stairs\"\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "#Load X and Y Lists from Pickle Files\n",
    "x_bfile = open('x.pickle', 'rb')\n",
    "x_data = pickle.load(x_bfile)/255.0\n",
    "\n",
    "y_bfile = open('y.pickle', 'rb')\n",
    "y_data_int = pickle.load(y_bfile)\n",
    "y_data = keras.utils.to_categorical(y_data_int, num_classes=3)\n",
    "\n",
    "#Extract Test Data\n",
    "no_train = int(len(x_data)*(1-test_data_percentage))\n",
    "img_size = len(x_data[0])\n",
    "x_train = x_data[:no_train]\n",
    "y_train = y_data[:no_train]\n",
    "y_train_int = y_data_int[:no_train]\n",
    "x_test = x_data[no_train:]\n",
    "y_test = y_data[no_train:]\n",
    "y_test_int = y_data_int[no_train:]\n",
    "\n",
    "#Configing to release GPU memory upon completion\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "print(\"Dataset Parsed\")\n",
    "print(\"Image Size:\", img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Visalisation\n",
    "Plots an image and its corresponding label to check if the data has been processed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_show = 301\n",
    "\n",
    "x_train_img = np.resize(x_train,(len(x_train), img_size, img_size))\n",
    "x_train_img  = x_train_img.astype(\"float32\")\n",
    "plt.imshow(x_train_img[img_to_show], cmap ='gray')\n",
    "plt.xlabel(class_names[y_train_int[img_to_show]], fontsize=18)\n",
    "print(\"x_train Shape:\", x_train.shape)\n",
    "print(\"y_train Shape:\", y_train.shape)\n",
    "print(\"y_train:\\n\", y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual Validation Split\n",
    "num_val = int(len(x_train)*0.1)\n",
    "x_val = x_train[:num_val]\n",
    "y_val = y_train[:num_val]\n",
    "x_train = x_train[num_val:]\n",
    "y_train = y_train[num_val:]\n",
    "\n",
    "#Training Datagen\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "train_datagen.fit(x_train)\n",
    "train_generator = train_datagen.flow(x_train, y_train, batch_size=32)\n",
    "\n",
    "#Validation Datagen\n",
    "val_datagen = keras.preprocessing.image.ImageDataGenerator() #no data augmentation\n",
    "val_generator = val_datagen.flow(x_val, y_val, batch_size=32)\n",
    "val_steps = int(num_val/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Create Model Using Inception-ResNet v2 (Keras Applications)\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "img_input = layers.Input(shape=(img_size,img_size,1))\n",
    "img_conc = layers.Concatenate()([img_input, img_input, img_input])    \n",
    "\n",
    "#Preparing the Base Model (299x299x3 input)\n",
    "base_model = InceptionResNetV2(input_tensor=img_conc,weights='imagenet', include_top=False)\n",
    "\n",
    "#Modifying the End of the Base Model\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "#Putting the model together \n",
    "model = keras.models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# 261 is the end of the 10th a block: block35_10_ac\n",
    "# 275 is the end of the a reduction block\n",
    "# 595 is the end of the 20th b block: block17_20_ac\n",
    "# 618 is the end of the b reduction block: mixed_7a\n",
    "# 777 is the end of the 10th c block: block8_10\n",
    "\n",
    "# for i, layer in enumerate(base_model.layers):\n",
    "#    print(i, layer.shape)\n",
    "\n",
    "# print(model.summary())\n",
    "# plot_model(model, to_file='model.png', dpi=200, show_shapes=True, show_layer_names=False)\n",
    "\n",
    "\n",
    "############ Training\n",
    "epoch_new = 5 #5\n",
    "epoch_bc= 20 #20\n",
    "\n",
    "#Train New Layers first\n",
    "print(\"Training NEW LAYERS\")\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.fit(x_train, y_train, epochs=epoch_new, validation_split=0.1)\n",
    "model.fit_generator(train_generator, validation_data=val_generator,steps_per_epoch=len(x_train) / 32,validation_steps=val_steps, epochs=epoch_new)\n",
    "\n",
    "#Train b and c blocks\n",
    "print(\"Training B and C\")\n",
    "for layer in model.layers[:275]: #275\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[275:]:\n",
    "    layer.trainable=True\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "_optimizer = Adam(lr=0.0001)\n",
    "model.compile(optimizer=_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#history = model.fit(x_train, y_train, epochs=epoch_bc, validation_split=0.1, callbacks=[tensorboard])\n",
    "history = model.fit_generator(train_generator, validation_data=val_generator,steps_per_epoch=len(x_train) / 32, validation_steps=val_steps, epochs=epoch_bc)\n",
    "print(\"Training Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save the Model\n",
    "model.save('stairs_model.h5')\n",
    "print(\"Model Saved\")\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load Model\n",
    "model = keras.models.load_model('stairs_model.h5')\n",
    "model.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"\\nAccuracy Stats\")\n",
    "print(\"Train Accuracy:\", round(history.history['acc'][-1]*10000)/100, \"%\")\n",
    "print(\"Validation Accuracy:\", round(history.history['val_acc'][-1]*10000)/100, \"%\")\n",
    "print(\"Test Accuracy:\", round(test_acc*10000)/100, \"%\")\n",
    "\n",
    "print(\"\\nLoss Stats\")\n",
    "print(\"Train Loss:\", round(history.history['loss'][-1]*1000)/1000)\n",
    "print(\"Validation Loss:\", round(history.history['val_loss'][-1]*1000)/1000)\n",
    "print(\"Test Loss:\", round(test_loss*1000)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Cross Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Cross Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print(\"Generated Predictions for Test Data\")\n",
    "print(\"Predictions Shape:\",predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing Predictions\n",
    "predict_img(i) shows predicted and actual class of image i in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Test Images\", len(x_test))\n",
    "x_test_img = np.resize(x_test, (len(x_test), img_size, img_size))\n",
    "x_test_img = x_test_img.astype('float32')\n",
    "def predict_img(i):\n",
    "    print(predictions[1])\n",
    "    plt.imshow(x_test_img[i], cmap=\"gray\")\n",
    "    plt.xlabel(\"Prediction: [\" + str(round(predictions[i][0]*100)/100.0) + \"][\" + str(round(predictions[i][1]*100)/100.0) + \"][\" + str(round(predictions[i][2]*100)/100.0) + \"] - \"+ class_names[np.argmax(predictions[i])]+ \", Actual Class: \" + class_names[y_test_int[i]], fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_img(93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
